{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f116aeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.0\n",
      "Keras version: 2.6.0\n",
      "Eager mode enabled: True\n",
      "Num GPUs Available:  0\n",
      "Num CPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 12:41:11.056922: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/17.02.11/lib64/slurm:/cm/shared/apps/slurm/17.02.11/lib64:/cm/local/apps/gcc/6.3.0/lib:/cm/local/apps/gcc/6.3.0/lib64\n",
      "2022-12-13 12:41:11.056994: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-13 12:41:11.057064: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (smcluster): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys; sys.path.insert(0, '..') # add parent folder path where lib folder is\n",
    "import lib.metrics\n",
    "import lib.load_data\n",
    "import lib.evaluate\n",
    "\n",
    "from lib.evaluate import testModel\n",
    "from lib.load_data import get_data\n",
    "from lib.metrics import iou_metric\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "print('TensorFlow version: {version}'.format(version=tf.__version__))\n",
    "print('Keras version: {version}'.format(version=tf.keras.__version__))\n",
    "print('Eager mode enabled: {mode}'.format(mode=tf.executing_eagerly()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec657a",
   "metadata": {},
   "source": [
    "# Metrics Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea229dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pic_(82).jpg', 'pic_(45).jpg', 'pic_(12).jpg', 'pic_(21).jpg', 'pic_(157).jpg', 'pic_(40).jpg', 'pic_(15).jpg', 'pic_(103).jpg', 'pic_(135).jpg', 'pic_(132).jpg', 'pic_(49).jpg', 'pic_(76).jpg', 'pic_(64).jpg', 'pic_(150).jpg', 'pic_(145).jpg', 'pic_(92).jpg', 'pic_(25).jpg', 'pic_(8).jpg', 'pic_(123).jpg', 'pic_(122).jpg', 'pic_(29).jpg']\n",
      "(21, 256, 256, 3)\n",
      "Getting and resizing images ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 21/21 [00:01<00:00, 18.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "(21, 256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "path_test_levee = \"../datasets/experiment_3/test\"\n",
    "test_images_leveecrack = sorted(next(os.walk(path_test_levee + \"/images\"))[2])\n",
    "\n",
    "random.shuffle(test_images_leveecrack)\n",
    "print(test_images_leveecrack)\n",
    "\n",
    "image_leveecrack_path = path_test_levee + \"/images\"\n",
    "mask_leveecrack_path = path_test_levee + \"/masks\"\n",
    "\n",
    "\n",
    "X_test_levee, Y_test_levee = get_data(test_images_leveecrack, path_test_levee, img_height, img_width, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345ba3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_for_dataset(X_test, Y_test, model_name, dataset_name, experiment_name):\n",
    "    img_height = 256\n",
    "    img_width = 256\n",
    "    \n",
    "    result_folder_name = str(model_name) + \"_\" + str(dataset_name)\n",
    "    root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "    model_name = model_name + \"_best_model.h5\"\n",
    "    model_path = os.path.join(root_dir,  \"models\", str(experiment_name), str(model_name))\n",
    "    tf.keras.backend.clear_session()\n",
    "    %time\n",
    "    best_model = tf.keras.models.load_model(model_path, compile=False)\n",
    "    print(f'=========Loaded {model_name} for {dataset_name}===========')\n",
    "    results = testModel(best_model, X_test, Y_test, 4, result_folder_name)\n",
    "    print(\"==========Evaluation Completed============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b8353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 µs, sys: 0 ns, total: 64 µs\n",
      "Wall time: 128 µs\n",
      "=========Loaded unet_best_model.h5 for levee===========\n",
      "6/6 [==============================] - 3s 364ms/step\n",
      "0.28178063\n",
      "0.41135877\n",
      "0.6005029 0.37175754 0.45922154\n",
      "Test Jacard Index : 0.28178063\n",
      "Test Dice Coefficient : 0.41135877\n",
      "Test iou_all :0.283764545143966\n",
      "Test tversky : 0.40045375\n",
      "Test f1_score : 0.45922154\n",
      "==========Evaluation Completed============\n",
      "CPU times: user 22 µs, sys: 11 µs, total: 33 µs\n",
      "Wall time: 66.3 µs\n",
      "=========Loaded multiresunet_best_model.h5 for levee===========\n",
      "6/6 [==============================] - 6s 809ms/step\n",
      "0.22503875\n",
      "0.33582193\n",
      "0.6208494 0.27700317 0.3830857\n",
      "Test Jacard Index : 0.22503875\n",
      "Test Dice Coefficient : 0.33582193\n",
      "Test iou_all :0.22709006429119402\n",
      "Test tversky : 0.31052116\n",
      "Test f1_score : 0.3830857\n",
      "==========Evaluation Completed============\n",
      "CPU times: user 32 µs, sys: 0 ns, total: 32 µs\n",
      "Wall time: 62.7 µs\n",
      "=========Loaded attentionunet_best_model.h5 for levee===========\n",
      "6/6 [==============================] - 5s 738ms/step\n",
      "0.30066627\n",
      "0.43031675\n",
      "0.5841285 0.4510125 0.5090113\n",
      "Test Jacard Index : 0.30066627\n",
      "Test Dice Coefficient : 0.43031675\n",
      "Test iou_all :0.3034657766745403\n",
      "Test tversky : 0.4291399\n",
      "Test f1_score : 0.5090113\n",
      "==========Evaluation Completed============\n",
      "CPU times: user 28 µs, sys: 14 µs, total: 42 µs\n",
      "Wall time: 87 µs\n",
      "=========Loaded nestedunet_best_model.h5 for levee===========\n",
      "6/6 [==============================] - 9s 1s/step\n",
      "0.30583933\n",
      "0.4381598\n",
      "0.61004525 0.41986609 0.49739686\n",
      "Test Jacard Index : 0.30583933\n",
      "Test Dice Coefficient : 0.4381598\n",
      "Test iou_all :0.30874640051427743\n",
      "Test tversky : 0.4302805\n",
      "Test f1_score : 0.49739686\n",
      "==========Evaluation Completed============\n",
      "CPU times: user 0 ns, sys: 50 µs, total: 50 µs\n",
      "Wall time: 83.7 µs\n",
      "=========Loaded iterlunet_best_model.h5 for levee===========\n",
      "6/6 [==============================] - 31s 5s/step\n",
      "0.34859768\n",
      "0.48746058\n",
      "0.49318317 0.538829 0.51499665\n",
      "Test Jacard Index : 0.34859768\n",
      "Test Dice Coefficient : 0.48746058\n",
      "Test iou_all :0.35109385921726805\n",
      "Test tversky : 0.5046354\n",
      "Test f1_score : 0.51499665\n",
      "==========Evaluation Completed============\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_for_dataset(X_test_levee, Y_test_levee, \"unet\", \"levee\", \"experiment_3\")\n",
    "evaluate_model_for_dataset(X_test_levee, Y_test_levee, \"multiresunet\", \"levee\" , \"experiment_3\")\n",
    "evaluate_model_for_dataset(X_test_levee, Y_test_levee, \"attentionunet\", \"levee\", \"experiment_3\")\n",
    "evaluate_model_for_dataset(X_test_levee, Y_test_levee, \"nestedunet\", \"levee\", \"experiment_3\")\n",
    "evaluate_model_for_dataset(X_test_levee, Y_test_levee, \"iterlunet\", \"levee\", \"experiment_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac5052",
   "metadata": {},
   "source": [
    "# Generate Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8c111",
   "metadata": {},
   "source": [
    "### Method for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd9a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_ids, image_path, mask_path, unet, multiresunet, attnunet, nestedunet, proposed):\n",
    "    ids = test_ids\n",
    "    \n",
    "    image_names, mask1, mask2, mask3, mask4, mask5,image_id, mask_id, has_mask = [], [], [], [], [], [],[], [], []\n",
    "    \n",
    "    image_path = os.path.abspath(image_path)\n",
    "    mask_path = os.path.abspath(mask_path)\n",
    "    \n",
    "    # Iterate through each images in test dataset\n",
    "    for n, i in tqdm(enumerate(ids), total=len(ids)):\n",
    "        filename = i\n",
    "        image_names.append(filename)\n",
    "        i = image_path + '/' + i\n",
    "\n",
    "        # Create a empty array of shape 1,256,256,1\n",
    "        X = np.empty((1,256,256,3))\n",
    "        # read the image\n",
    "        img = io.imread(i)\n",
    "        # Resize the image and coverting them to array of type float64\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = np.array(img)\n",
    "        \n",
    "        # Strandardize image similar to that used in the training process\n",
    "        img = img / 255\n",
    "        \n",
    "        X[0,] = img\n",
    "        \n",
    "        # Prediction of masks\n",
    "        predict1 = unet.predict(X)\n",
    "        predict2 = multiresunet.predict(X)\n",
    "        predict3 = attnunet.predict(X)\n",
    "        predict4 = nestedunet.predict(X)\n",
    "        predict5 = proposed.predict(X)\n",
    "        \n",
    "        image_id.append(image_path + '/' + filename)\n",
    "        mask_id.append(mask_path + '/' + format(filename.split('.')[0]+'.png'))\n",
    "        has_mask.append(1)\n",
    "    \n",
    "        mask1.append(predict1)\n",
    "        mask2.append(predict2)\n",
    "        mask3.append(predict3)\n",
    "        mask4.append(predict4)\n",
    "        mask5.append(predict5)\n",
    "            \n",
    "            \n",
    "    return pd.DataFrame({'file_name':image_names,'image_path': image_id, 'mask_path': mask_id,  'UNet_mask': mask1, 'MultiresUNet_mask': mask2, 'AttentionUNet_mask': mask3,\n",
    "                         'NestedUNet_mask': mask4, 'IterLUNet_mask':mask5, 'has_mask': has_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc8b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true_in, y_pred_in):\n",
    "    smooth = 1e-15\n",
    "    intersection = y_true_in.ravel() * y_pred_in.ravel()\n",
    "    union = y_true_in.ravel() + y_pred_in.ravel() - intersection\n",
    "\n",
    "    iou = ((np.sum(intersection) + smooth)/(np.sum(union) + smooth))\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3bc17",
   "metadata": {},
   "source": [
    "### Method for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f529a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_visualization(df_pred, filename, img_width, img_height):\n",
    "    count = 0\n",
    "    fig, axs = plt.subplots(8,7, figsize=(62, 78)) # (width, height)\n",
    "    dim = (img_width,img_height)\n",
    "    \n",
    "    root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "    filepath = os.path.join(root_dir, \"results\", filename)\n",
    "    \n",
    "    for i in range(len(df_pred)):\n",
    "        if df_pred.has_mask[i]==1 and count<8:\n",
    "            \n",
    "            #read levee crack image\n",
    "            img = cv2.imread(df_pred.image_path[i], cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, dim)\n",
    "            axs[count][0].imshow(img)\n",
    "            axs[count][0].set_axis_off() \n",
    "            axs[count][0].set_title('Original Image ' + str(count+1), fontsize=78)\n",
    "    \n",
    "    \n",
    "            \n",
    "            #read original mask and overlay original mask with levee crack image    \n",
    "            mask = cv2.imread(df_pred.mask_path[i], cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, dim)\n",
    "            \n",
    "            img[mask==255] = (255,0,0)\n",
    "            axs[count][1].imshow(img)\n",
    "            axs[count][1].set_axis_off()\n",
    "            axs[count][1].set_title('Ground Truth '+ str(count+1), fontsize=78)\n",
    "            mask = np.array(mask, dtype = np.float64)\n",
    "            mask = mask / 255\n",
    "            \n",
    "            #for U-Net (M1)\n",
    "            pred_unet = np.array(df_pred.UNet_mask[i])\n",
    "            iou_unet = iou_metric(mask, pred_unet )\n",
    "            pred_unet = np.round(pred_unet,0).squeeze()\n",
    "            img1 = cv2.imread(df_pred.image_path[i], cv2.IMREAD_COLOR)\n",
    "            img1 = cv2.resize(img1, (256, 256))\n",
    "            img1 = img1.squeeze()\n",
    "            img1[pred_unet==1] = (255, 255, 255)\n",
    "            axs[count][2].imshow(img1)\n",
    "            axs[count][2].set_axis_off()\n",
    "            axs[count][2].set_title('[IoU M1=' + str(round(iou_unet,2)) + ']', fontsize=78)\n",
    "            \n",
    "            \n",
    "            #for MultiResUNet (M2)\n",
    "            pred_multiresunet = np.array(df_pred.MultiresUNet_mask[i])\n",
    "            iou_multiresunet = iou_metric(mask, pred_multiresunet )\n",
    "            pred_multiresunet = np.round(pred_multiresunet,0).squeeze()\n",
    "            img2 = cv2.imread(df_pred.image_path[i], cv2.IMREAD_COLOR)\n",
    "            img2 = cv2.resize(img2, (256,256))\n",
    "            img2 = img2.squeeze() / 255\n",
    "            img2[pred_multiresunet==1] = (255, 255, 255)\n",
    "            #axs[count][3].imshow((img2 * 255).astype(np.uint8))\n",
    "            axs[count][3].imshow(img2)\n",
    "            axs[count][3].set_axis_off()\n",
    "            axs[count][3].set_title('[IoU M2=' + str(round(iou_multiresunet, 2)) + ']', fontsize=78)\n",
    "            \n",
    "            \n",
    "            #for Attention UNet (M3)\n",
    "            pred_attentionunet = np.array(df_pred.AttentionUNet_mask[i])\n",
    "            iou_attentionunet = iou_metric(mask, pred_attentionunet )\n",
    "            pred_attentionunet = np.round(pred_attentionunet,0).squeeze()\n",
    "            img3 = cv2.imread(df_pred.image_path[i], cv2.IMREAD_COLOR)\n",
    "            img3 = cv2.resize(img3, (256,256))\n",
    "            img3 = img3.squeeze() / 255\n",
    "            img3[pred_attentionunet==1] = (255, 255, 255)\n",
    "            axs[count][4].imshow(img3)\n",
    "            axs[count][4].set_axis_off() \n",
    "            axs[count][4].set_title('[IoU M3=' + str(round(iou_attentionunet, 2)) + ']', fontsize=78)\n",
    "            \n",
    "            \n",
    "            #for UNet++ (M4)\n",
    "            pred_nestedunet = np.array(df_pred.NestedUNet_mask[i])\n",
    "            iou_nestedunet = iou_metric(mask, pred_nestedunet )\n",
    "            pred_nestedunet = np.round(pred_nestedunet,0).squeeze()\n",
    "            img4 = cv2.imread(df_pred.image_path[i], cv2.IMREAD_COLOR)\n",
    "            img4 = cv2.resize(img4, (256, 256))\n",
    "            img4 = img4.squeeze()\n",
    "            img4[pred_nestedunet==1] = (255, 255, 255)\n",
    "            axs[count][5].imshow(img4)\n",
    "            axs[count][5].set_axis_off() \n",
    "            axs[count][5].set_title('[IoU M4=' + str(round(iou_nestedunet, 2)) + ']', fontsize=78)\n",
    "            \n",
    "            \n",
    "            #for IterLUNet (M5)\n",
    "            pred_inception = np.array(df_pred.IterLUNet_mask[i])\n",
    "            iou_inception = iou_metric(mask, pred_inception )\n",
    "            pred_inception = np.round(pred_inception,0).squeeze()\n",
    "            img5 = cv2.imread(df_pred.image_path[i], cv2.IMREAD_COLOR)\n",
    "            img5 = cv2.resize(img5, dim)\n",
    "            img5 = img5.squeeze() \n",
    "            img5[pred_inception==1] = (0, 0, 255)\n",
    "            axs[count][6].imshow(img5)\n",
    "            axs[count][6].set_axis_off() \n",
    "            axs[count][6].set_title('[IoU M5=' + str(round(iou_inception, 2)) + ']', fontsize= 78)\n",
    "            \n",
    "            \n",
    "            fig.subplots_adjust(top=0.94)\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(filepath, format='png', facecolor=\"w\", transparent=False)\n",
    "        \n",
    "            count +=1\n",
    "        if (count==8):\n",
    "            break\n",
    "    fig.tight_layout() \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a67751",
   "metadata": {},
   "source": [
    "### Method to load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb22967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, experiment_name):\n",
    "    result_folder_name = str(model_name)\n",
    "    root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "    model_name = model_name + \"_best_model.h5\"\n",
    "    model_path = os.path.join(root_dir,  \"models\", str(experiment_name), str(model_name))\n",
    "    tf.keras.backend.clear_session()\n",
    "    %time\n",
    "    best_model = tf.keras.models.load_model(model_path, compile=False)\n",
    "    print(f'=========Loaded {model_name}===========')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0fffa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 µs, sys: 19 µs, total: 37 µs\n",
      "Wall time: 72 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 12:41:30.177871: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Loaded unet_best_model.h5===========\n",
      "CPU times: user 20 µs, sys: 21 µs, total: 41 µs\n",
      "Wall time: 81.8 µs\n",
      "=========Loaded multiresunet_best_model.h5===========\n",
      "CPU times: user 17 µs, sys: 15 µs, total: 32 µs\n",
      "Wall time: 62.7 µs\n",
      "=========Loaded attentionunet_best_model.h5===========\n",
      "CPU times: user 0 ns, sys: 45 µs, total: 45 µs\n",
      "Wall time: 86.1 µs\n",
      "=========Loaded nestedunet_best_model.h5===========\n",
      "CPU times: user 17 µs, sys: 14 µs, total: 31 µs\n",
      "Wall time: 61.3 µs\n",
      "=========Loaded iterlunet_best_model.h5===========\n"
     ]
    }
   ],
   "source": [
    "unet = load_model('unet', 'experiment_3')\n",
    "multiresunet = load_model('multiresunet', 'experiment_3')\n",
    "attention_unet = load_model('attentionunet', 'experiment_3')\n",
    "nested_unet = load_model('nestedunet', 'experiment_3')\n",
    "iterlunet = load_model('iterlunet', 'experiment_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659f5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]2022-12-13 12:41:38.296673: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2aadad44fd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 21/21 [00:52<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "df_all_preds_all = []\n",
    "df_all_preds_all = prediction(test_images_leveecrack, os.path.abspath(image_leveecrack_path), os.path.abspath(mask_leveecrack_path), unet, multiresunet, attention_unet, nested_unet, iterlunet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f1fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "df_for_visualization = df_all_preds_all.sample(frac = 0.98)\n",
    "show_visualization(df_for_visualization, \"iee_levee_inferences.png\", img_width, img_height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-tf",
   "language": "python",
   "name": "gpu-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
